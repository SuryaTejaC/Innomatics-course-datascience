{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a94c6f9",
   "metadata": {},
   "source": [
    "- CNN on MNIST data\n",
    "\n",
    "- MNIST is hand written digits classification data, which has numbers from 0 to 9 adn we have 60000 training images and 10000 test images and these can be loaded from keras (no need of csv :) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22579e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a161429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21a98550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(X_train,y_train),(X_test,y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02e1254a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e804095",
   "metadata": {},
   "source": [
    "- For implementing NN we used something called Sequential for adding layer after layer.\n",
    "- For ANN -- what ever the input is we made into rows and columns 60000 X 28 X 28 , we made it 60000 X 784\n",
    "- we understood ANN's are not useful / not needed for images we use CNN.\n",
    "- We want to tell the sequential model that it should process images in the above output something is missing?\n",
    "- we have 60000 training examples\n",
    "- we have height = 28\n",
    "- we have width = 28 \n",
    "- what is missing? depth is missing\n",
    "- for processing images we need to specify that its n_e , n_h, n_w, and n_c\n",
    "- here the channel is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55178133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0],28,28,1)\n",
    "X_test = X_test.reshape(X_test.shape[0],28,28,1)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6bfd526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#like we did last time we will normalize the data\n",
    "# we will convert our target as one hot\n",
    "X_train= X_train.astype('float')\n",
    "X_test = X_test.astype('float')\n",
    "X_train/=255\n",
    "X_test/=255\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train,num_classes=10)\n",
    "y_test = to_categorical(y_test,num_classes=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dd464a",
   "metadata": {},
   "source": [
    "- Initialize a Sequential model\n",
    "- we will add 2 convolution layers with no of kernels (filters) = 32  (hyper)\n",
    "- kernel size = 3 X 3 (hyper)\n",
    "- activation = relu (hyper)\n",
    "- we will flatten the data \n",
    "- we will add one dense layer (128) (hyper), activation = relu (hyper)\n",
    "- we will add last layer with softmax with 10 neurons (not hyper parameter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81c39f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32,kernel_size=(3,3),activation='relu',input_shape=(28,28,1)))\n",
    "model.add(Conv2D(filters=32,kernel_size=3,activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95c65f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               2359424   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 2,370,282\n",
      "Trainable params: 2,370,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a45861ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d315f765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.optimizer_v2.adam.Adam at 0x28e30367610>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a7c0d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1313/1313 [==============================] - 64s 48ms/step - loss: 0.1321 - accuracy: 0.9583 - val_loss: 0.0640 - val_accuracy: 0.9793\n",
      "Epoch 2/10\n",
      "1313/1313 [==============================] - 62s 47ms/step - loss: 0.0393 - accuracy: 0.9878 - val_loss: 0.0636 - val_accuracy: 0.9813\n",
      "Epoch 3/10\n",
      "1313/1313 [==============================] - 65s 50ms/step - loss: 0.0238 - accuracy: 0.9920 - val_loss: 0.0654 - val_accuracy: 0.9819\n",
      "Epoch 4/10\n",
      "1313/1313 [==============================] - 66s 50ms/step - loss: 0.0155 - accuracy: 0.9947 - val_loss: 0.0591 - val_accuracy: 0.9846\n",
      "Epoch 5/10\n",
      "1313/1313 [==============================] - 69s 52ms/step - loss: 0.0100 - accuracy: 0.9966 - val_loss: 0.0642 - val_accuracy: 0.9849\n",
      "Epoch 6/10\n",
      "1313/1313 [==============================] - 71s 54ms/step - loss: 0.0110 - accuracy: 0.9960 - val_loss: 0.0732 - val_accuracy: 0.9843\n",
      "Epoch 7/10\n",
      "1313/1313 [==============================] - 62s 47ms/step - loss: 0.0073 - accuracy: 0.9977 - val_loss: 0.0735 - val_accuracy: 0.9837\n",
      "Epoch 8/10\n",
      "1313/1313 [==============================] - 63s 48ms/step - loss: 0.0064 - accuracy: 0.9979 - val_loss: 0.0693 - val_accuracy: 0.9869\n",
      "Epoch 9/10\n",
      "1313/1313 [==============================] - 65s 50ms/step - loss: 0.0036 - accuracy: 0.9991 - val_loss: 0.0813 - val_accuracy: 0.9859\n",
      "Epoch 10/10\n",
      "1313/1313 [==============================] - 63s 48ms/step - loss: 0.0069 - accuracy: 0.9977 - val_loss: 0.0714 - val_accuracy: 0.9867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23886bd77f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compiling the model\n",
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\n",
    "model.fit(X_train,y_train,batch_size=32,epochs=10,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd3d8ef",
   "metadata": {},
   "source": [
    "## Small assignment on CNN\n",
    "\n",
    "- define a sequential model\n",
    "- add 2 convolution layers\n",
    "- filters 32\n",
    "- size 3X3\n",
    "- activation = relu\n",
    "- add a maxpooling of size 2 X 2 \n",
    "- add a dropout layer with 0.2 \n",
    "- flatten\n",
    "- dense with 128 and relu\n",
    "- dense with 10 and softmax\n",
    "- compile and fit\n",
    "- try using early stopping \n",
    "- Let us know your findings after implementing the max pool and dropouts? what is the difference that you observed between my implementation and your assignment? \n",
    "- Evaluate on training and testing and see the difference?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4ad84e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import MaxPooling2D\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten\n",
    "from tensorflow.keras import optimizers,regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "993c3d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential([MaxPooling2D(pool_size = 2, strides = 2)])\n",
    "model2.add(Conv2D(filters=32,kernel_size=(3,3),activation='relu',input_shape=(28,28,1)))\n",
    "model2.add(Conv2D(filters=32,kernel_size=3,activation='relu'))\n",
    "model2.add(layers.Dropout(0.2))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(128,activation='relu'))\n",
    "model2.add(Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ff98f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09bde9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1313/1313 [==============================] - 18s 13ms/step - loss: 0.1881 - accuracy: 0.9420 - val_loss: 0.0915 - val_accuracy: 0.9704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28e26801d60>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\n",
    "model2.fit(X_train,y_train,batch_size=32,epochs=10,validation_split=0.3,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a89038ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.0799 - accuracy: 0.9751\n",
      "[0.07990290224552155, 0.9751499891281128]\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0782 - accuracy: 0.9751\n",
      "[0.07819437235593796, 0.9750999808311462]\n"
     ]
    }
   ],
   "source": [
    "print(model2.evaluate(X_train,y_train))\n",
    "print(model2.evaluate(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31237a02",
   "metadata": {},
   "source": [
    "# observation: in the above model without using maxpool,dropout,early stopping the model took 4 epochs to reach max accuracy and from 5th epoch without early stopping the model didn't improved much and waste of resources  but in model2 by using maxpool,dropout,early stopping the model reached the max accuracy with min loss and the model stopped early and saved resources by not executing up to all epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f6f270",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
